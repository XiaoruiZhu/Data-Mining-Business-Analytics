---
title: "Introduction to the Data Cleaning"
output:
  html_document: 
    fig_caption: yes
    number_sections: yes
    theme: readable
    toc: yes
    code_folding: show
    df_print: paged
  html_notebook:
    toc: yes
  md_document:
    variant: markdown_github
  pdf_document: default
editor_options: 
  chunk_output_type: console
---

```{r setup, echo=FALSE, include=FALSE}
knitr::opts_chunk$set(eval = TRUE, warning=FALSE, message = FALSE)
```


# Most Time-Consuming Data Science Task: Data Cleaning (80% of the work)

[**Data preparation** accounts for about 80% of the work of data scientists](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/#6a089d126f63)

![](figure/DataPreparation.jpg){width=80%}

> **Most Time-Consuming Data Science Task**
>
> Question: Do you enjoy data cleaning? 

Data cleaning is a crucial step in the process of preparing data for analysis in the field of business analytics. It involves identifying and correcting errors, inconsistencies, and inaccuracies in the dataset to ensure its **reliability** and **quality**. 

In business analytics, data comes from various sources such as databases, spreadsheets, and external sources. These sources often contain missing values, duplicate entries, outliers, and formatting inconsistencies, which can strongly skew the analysis results and lead to incorrect business decisions if not addressed properly.

- Example 1

The following two datasets are the raw and clean data. The Raw dataset contains the headers and other information that triggers missing values. Without cleaning, it cannot be directly used. On the contrary, the clean dataset presented right after has the rows and colomns clearly stroed. 

```{r cols.print=3, rows.print=3}
# install.packages("readxl")
Retail <- readxl::read_xlsx(path = "data/RetailSales2018.xlsx")
head(Retail)
Retail_clean <- readxl::read_xlsx(path = "data/RetailSales2018_clean.xlsx")
Retail_clean
```

Data sources: [Retail Sales Raw Data](https://xiaorui.site/Data-Mining-Business-Analytics/lab/data/RetailSales2018.xlsx) and [Retail Sales Clean Data](https://xiaorui.site/Data-Mining-Business-Analytics/lab/data/RetailSales2018_clean.xlsx)


```{r echo=FALSE, eval=FALSE, cols.print=3, rows.print=3}
# Store the data in a conveniently named variable
Sales <- Retail_clean$Sales
# Figure 1.2
plot(Sales, type = "l")

# head(Retail_clean)
# Change to Time-Series Object
library(zoo)
Retail_Sales <- ts(Retail_clean$Sales, start = as.yearmon("1992-01"), freq = 12)
head(Retail_Sales)

# install.packages("forecast")
require("forecast")
plot(Retail_Sales)

# Produce the seasonal plot of the last 4 years. We use the function seasonplot from the forecast package
z <- window(Retail_Sales, start=c(2011,1))
seasonplot(z)
```

```{r, eval=FALSE, echo=FALSE}
# Figure 1.3
Sales_adj <- Retail_clean$Sales / Retail_clean$S_Factor
plot(Sales_adj, type = "l")

# Figure 1.5
Sales_MOM <- (Sales_adj-quantmod::Lag(Sales_adj, k=1))/quantmod::Lag(Sales_adj, k=1)
Sales_MOM2 <- ts(Sales_MOM, start = as.yearmon("1992-01"), freq = 12)
Sales_MOM3 <- window(Sales_MOM2, start=c(2001,1))
plot(Sales_MOM3, type = "l")

# Figure 1.6
Sales_YOY <- (Sales_adj-quantmod::Lag(Sales_adj, k=12))/quantmod::Lag(Sales_adj, k=12)
Sales_YOY2 <- ts(Sales_YOY, start = as.yearmon("1992-01"), freq = 12)
Sales_YOY3 <- window(Sales_YOY2, start=c(2001,1))
plot(Sales_YOY3, type = "l")
```


# Goal of Data Cleaning

The goal of data cleaning is to transform raw data into a clean, consistent, and reliable dataset that can be used for accurate analysis and decision-making. This process typically includes several steps:

1. **Data Profiling**: Examining the dataset to understand its structure, variables, and distribution of values. This helps identify potential issues such as missing values, outliers, and inconsistencies.

2. **Handling Missing Values**: Dealing with missing data by imputing values based on statistical methods or business knowledge. This ensures that the dataset remains complete and representative.

3. **Removing Duplicates**: Identifying and removing duplicate entries to avoid redundancy and maintain data integrity.

4. **Standardizing Data**: Ensuring consistency in data formats, units, and scales across variables to facilitate analysis and comparison.

5. **Handling Outliers**: Identifying outliers that may distort analysis results and deciding whether to remove them or transform them appropriately.

6. **Data Validation**: Checking data integrity by validating values against predefined rules or business logic to ensure accuracy and reliability.

7. **Data Transformation**: Transforming variables or creating new features to better represent the underlying relationships and patterns in the data.

8. **Documentation**: Documenting the data cleaning process, including the steps taken and any decisions made, to ensure transparency and reproducibility.

By performing thorough data cleaning, business analysts can improve the quality and reliability of their analyses, leading to more accurate insights and better-informed business decisions. It lays the foundation for successful data analysis and ensures that businesses can trust the results derived from their data-driven initiatives.

[go to top](#header)